
% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- introduction file header -----------------------
% the code below specifies where the figures are stored
\graphicspath{{1/figures/}}

\chapter{Introduction}
\label{chp:introduction}

% Information age
It goes without saying that we live in the Age of Information, our day to day experiences awash in a flood of data.
As a society, we buy, sell, consume and produce information in unprecedented quantities.
% More data than anyone can handle
Given the accelerating rate at which information is created, one of the fundamental challenges facing the modern world is simply making sense of all this data.
% Google bitches
The quintessential response to this obstacle is embodied by Google, whose collective \emph{raison d'\^etre} is the organization and indexing of the world's information.
To appreciate the value and reach of this technology, one only needs to imagine how difficult it would be to browse the Internet without a search engine.

 % is a significant research effort behind the development of computational methods and algorithms to help make information universally accessible and useful. %, and this is particularly evident in the realm of music.

% Information overload, yo

% This domain is broadly referred to as \emph{information retrieval} (IR), and has grown rapidly in recent years, with various subdomains coalescing around application-specific topics.

Understandably, a variety of specialized disciplines have formed under the auspices of developing systems to help people navigate and understand massive amounts of information.
Coalescing around the turn of the century, music informatics is one such instance, drawing from several diverse fields including electrical engineering, music psychology, computer science, machine learning, and music theory, among others.
Now encompassing a wide spectrum of application areas and the kinds of data considered---from audio and text to album covers and online social interactions---music informatics can be broadly defined as the study of information related to, or is a result of, musical activity.

% IR Formulation
At a high level, tackling this problem of ``information overload'' in music is captured by a simple, general analogy: how exactly \emph{does} one find a needle in a haystack?
To answer this question, any system, computational or otherwise, must solve two related problems:
first, it is necessary to describe the intrinsic qualities of the item of interest, e.g. a needle is metal, sharp, thin, etc;
and second, it is necessary to evaluate the extrinsic relationships between items to determine relevance.
A piece of hay is certainly not a needle, for example, but is a pin close enough?
Along what criteria might we gauge similarity, or classify objects into groups?
Emphasizing the distinction, \emph{description} focuses on absolute representation, whereas \emph{comparison} is concerned with relative associations.

% Human provided descriptions and relationships
To date, the most successful approaches to large-scale information systems leverage human-provided signals to achieve rich content descriptions.
Building on top of robust representations simplifies the problem greatly, and good progress has been made toward the development of useful applications.
% Human signals about the content
For example, the Netflix Prize challenge\footnote{\url{http://www.netflixprize.com/}} ---an open contest to find the best system for automatically predicting a user's enjoyment of a movie--- was built exclusively on movie ratings contributed by a large collection of other users.
% Links between content
Similarly, Google's \emph{PageRank} algorithm associates websites based on how users have linked different pages together, thus facilitating the process of traversing the Internet \cite{Page1999Pagerank}.

% Music is a different ballgame, y'all; document description is wicked hard.
While this strategy of leveraging manual content description has proven successful in large-scale music recommendation, such as Pandora Radio\footnote{\url{http://www.pandora.com/}}, its application to more general music information problems is fundamentally limited, manifesting in three related ways.
First, human-provided information commonly used in such systems ---clicks, likes, listens or shares--- are easily captured from, or as a natural by-product of, a user's listening to music.
It is one thing to obtain a ``thumbs up'' for a song; it is quite another to ask that same user to provide a chord transcription of it.
Second, manual music description may require a high degree of expertise or effort to perform.
The average music listener is not truly capable of transcribing chords from a sound recording, whether or not she possesses the time or willingness to attempt it.
Finally, even given the skill, motivation, and infrastructure to manually describe music, this approach cannot scale to \emph{all} music content, now or in the future.
The Music Genome Project\footnote{\url{https://www.pandora.com/about/mgp}}, for example, has resulted in the manual annotation of some 1M commercial recordings, at a pace of 20-30 minutes per track;
the iTunes Music Store, however, now offers over 43M tracks worldwide\footnote{According to \url{http://www.apple.com/itunes/music/}, accessed 20 April, 2015.}.
To illustrate how vast this discrepancy is, consider the following: even assuming the lower bound of 20 minutes, it would still take one sad individual \emph{1,600 years} of non-stop annotation to close that gap.
More importantly, this only considers commercial music recordings, neglecting amateur or unpublished content from websites like YouTube\footnote{https://www.youtube.com/} or Soundcloud\footnote{\url{https://soundcloud.com/}}, the addition of which makes this goal even more insurmountable.
Given the sheer impossibility for humans to meaningfully describe all recorded music, now and in the future, truly scalable music information systems will require good automatic systems to perform this task.

% Music description is good, state of the art is bad
Thus, the development of computational systems to describe music signals, a flavor of \emph{computer audition} referred to as content-based music informatics, is both a valuable and fascinating problem.
In addition to facilitating the search and retrieval of large music collections, automatic systems capable of expert-level music description are invaluable to users who are unable to perform the task themselves, e.g. music transcription.
Notably, this problem is also very much unsolved, and given an apparent deceleration of progress, some in the field of music informatics have begun to question the efficacy of traditional research methods.
% Statement of the issues?
Simultaneously, in the related fields of computer vision and automatic speech recognition, a branch of machine learning, referred to as \emph{deep learning}, has shown great performance in various domains, toppling many long-standing benchmarks.
On closer inspection, one recognizes considerable conceptual overlap between deep learning and conventional music signal processing systems, further encouraging this promising union.

% Research goal
Synthesizing these observations, this study explores deep learning as a general approach to the design of computer audition systems for music description applications.
More specifically, the proposed research method proceeds thusly:
first, methods and trends in content-based music informatics are reviewed in an effort to understand why progress in this domain may be decellerating, and, in doing so, identify possible deficiencies in this methodology;
standard approaches to music signal processing are then reformulated in the language and concepts of deep learning, and subsequently applied to classic music informatics problems;
finally, the behavior of these deep learning systems is deconstructed in order to illustrate the advantages and challenges inherent to this paradigm.


\section{Scope of this Study}
\label{sec:scope}

% Functionally model intelligent behavior
% how to judge whether or not the description is good? and what is good?
% hinges on how quality is defined
% assessment is driven by how the system behaves compared to

This study explores the use of deep learning in the development of systems for automatic music description.
Consistent with the larger body of machine perception research, the work presented here aims to computationally model the relationship between stimuli and observations made by an intelligent agent.
In this case, ``stimuli'' are digital signals representing acoustic waves, e.g. sound, ``observations'' are semantic descriptions in a particular namespace, e.g. timbre or harmony, and the agent being modeled is an intelligent human, e.g. an expert music listener.
In practice, the namespace of descriptions considered is constrained to a particular task or application, such as instrument recognition or chord estimation.

Furthermore, if the relationship between stimuli and observation is not a function of the agent, this mapping is said to be ``objective''.
Objective relationships are those that are true absolutely by definition, such as the statement ``A C Major triad consists of the pitches C, E, and G.''
Elaborating, all sufficiently capable agents should always produce the same output given the same input.
Discrepancies between observations of the same stimuli are understood as one or more of these perspectives being erroneous, resulting from either simple error, bias, or a deficiency of knowledge.
For objective relationships, the \emph{quality} of a model is determined by how often it is able to produce ``right'' answers, often referred to as ``ground truth'', to the questions being asked.

Conversely, input-output relationships that \emph{are} a meaningful function of the agent are said to be ``subjective''.
In contrast to the objective case, which is fundamentally concerned with \emph{facts}, a subjective observation is ultimately an \emph{opinion}.
As such, an opinion can only be true or false insofar as it is held by a competent agent.
This is embodied, for example, in the statement ``That sounds like a saxophone.''
Whether or not the stimuli originated from a saxophone is actually irrelevant;
a rational agent has made the observation, and thus it is in some sense valid.
Assessing the quality of a computational model at a subjective task must therefore take one of two slightly different formulations.
The first transforms a subjective problem to an objective one by considering the perspective of single agent as truth, and thus the quality of a model is a function of how well it can mimic the behavior of that \emph{one} agent.
Alternatively, the other approach attempts to determine whether or not a model makes observations on par with other competent agents. % e.g. does a system \emph{behave} like other intelligent agents.
In this view, a computational system's capacity to perform some intelligent task is measured by its ability to convince humans that it is competent (or not) in human ways, e.g. the Turing test \cite{Turing1950Computing}.

The notions of, and inherent conflict between, objectivity and subjectivity in audition and music perception are central to the challenge posed by the computational modeling of it.
Arguably most facets of music perception are subjective and vary in degree from task to task.
However, while subjective evaluation might be better suited toward measuring the quality or usability of some computational system, the human involvement required by such assessments make them prohibitively costly in both time and money to conduct with any regularity.
As a result, conventional research methodology in engineering and computer science greatly prefers \emph{quantitative} evaluation as a proxy to \emph{qualitative} responses collected from human subjects.
Typically quantitative methods proceed by collecting some number of input-output pairs from one or more human subjects beforehand, and treating this data sample as objective truth.
Thus, regardless of whether or not a given task is indeed objective, it is a significant simplification in methodology to treat it as one.

This is all to say that the validity and quality of a music description is often determined by an objective fitness measure, not necessarily out of correctness but rather tractability.
Therefore, any quantitative measure is only valid insofar as the assumption of objectivity is as well.


\section{Motivation}
% Significance


The proposed research is primarily motivated by two complementary observations:
one, large scale music signal processing systems are becoming necessary to help humans navigate and make sense of an ever-increasing volume of music information;
two ---and, more notably, the specific problem this work seeks to address--- the conventional research tradition in content-based music information retrieval is yielding diminishing returns, despite many research areas remaining unsolved.

In the most immediate sense, the proposed research will develop systems to tackle various applications in music informatics.
This will at least serve to explore an alternative approach to conventional problems in the field.
Based on preliminary results, there is good reason to believe that deep learning may in fact push the state of the art in some, if not most, applications in automatic music description.
Sufficiently advanced systems could be deployed in end-user applications, such as navigating music libraries or computer-aided composition and performance.

A thorough exploration and successful extension of deep learning to music signal processing has the potential to encourage a broader study of these methods.
The impacts of such a development could be far reaching, but there are two of particular note.
First and foremost, drawing attention to a promising, but otherwise uncharted, research area opens new opportunities for fresh ideas and perspectives.
Additionally, deep learning automatically optimizes a system to the data on hand, accelerating research and simplifying the overall design problem.
Therefore these methods yield flexible systems that can easily adapt to new data as well as new problems, allowing researchers to seek out novel, exciting applications.

Beyond the scope of music informatics, deep learning research in the context of a different domain, with its own unique challenges, is likely to produce discoveries beneficial to the broad audience of computer science and information processing.
One such area where this is likely to occur is in the handling of time-domain signals and sequences.
Computer vision, the field in which most breakthroughs in deep learning have occurred, has invested considerable effort in the study of static, 2D images.
Certainly some have extended these techniques to image sequences and video, but this is far more the exception than the rule.
Other sequential data, such as natural language, in the form of text, speech signals, and motion capture data have also seen a deal of study in deep learning circles.
The tradition of music signal processing draws heavily from digital signal theory, a field of study with a considerable focus on an analytical understanding of time.

Therefore, this work offers several potential contributions, both theoretical and practical, to a diverse audience, spanning users of technology, music informatics, and the deep learning community on the whole.


\section{Dissertation Outline}
\label{sec:outline}
\begin{description}

\item Chapter \ref{chp:context} reviews the current state of affairs in music informatics research, providing context for this work.

\item Chapter \ref{chp:deep_learning} surveys the body of literature in deep learning, outlining core concepts and definitions.

\item Chapter \ref{chp:timbre} explores the application of deep learning toward the development of objective timbre similarity spaces.

\item Chapter \ref{chp:chord_estimation} considers the application of deep learning toward automatic chord estimation, as a means to both improve the state of the art and better understand the task at hand.

\item Chapter \ref{chp:guitar} extends this chord estimation efforts to directly estimate human-readable representations in the form of guitar tablature.

\item Chapter \ref{chp:reproducibility} documents the software contributions resulting from this study, contributing to the greater cause of reproducible research efforts.

\item Chapter \ref{chp:conclusion} concludes this thesis, summarizing the work presented and offering perspectives for future work and outstanding challenges.
% There will be tables and chairs, there'll be pony rides and dancing bears, there'll even be a band.

\end{description}

\section{Contributions}
The primary contributions of this dissertation are listed below:

\begin{itemize}
  \onehalfspacing
\item \textbf{Demonstrates an objective approach to the development of timbre similarity embeddings.} The proposed approach extends previous efforts in using pairwise training of deep architectures by relaxing constraints on the output space and generalizing the use of margins as a ratio, rather than an absolute parameter; in addition to realizing a far more discriminative instrument embedding than a shallow comparison system overall, the margin ratio improves performance slightly over the original pairwise training approach.
\item \textbf{Advances the state of the art in large vocabulary automatic chord estimation, while illustrating methodological limitations in the current formulation of the task.} Comprehensive error analysis is performed both by comparing two state of the systems against the reference chord transcriptions, as well as the proposed system against another dataset with multiple annotators. The insight gleaned from this study is used to offer perspective on future directions for the task at large.
\item \textbf{Leveraged traditional chord transcriptions to develop an automatic guitar chord estimation system, which directly maps music audio to fingerings on a fretboard.} Not only does this approach improve some measures over the other large-vocabulary chord estimation system presented here, but it provides a user-friendly interface for both learning and soliciting feedback on system errors.
\item \textbf{Contributes, in whole or part, to several open source projects to facilitate future efforts.} In addition to a suite of tools that may help serve the larger research community, this includes a framework to reproduce the experimental results and analysis contained herein.
\end{itemize}


\section{Associated Publications by the Author}

This thesis covers much of the work presented in the publications listed below:

\subsection{Peer-Reviewed Articles}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\vspace{1em}
\begin{itemize}
\onehalfspacing
\item Humphrey, E. J., Bello, J. P., and LeCun, Y. (2013) ``Feature learning and deep architectures: new directions for music informatics.''{\it Journal of Intelligent Information Systems}. 41 (3), 461--481.

\end{itemize}

\subsection{Peer-Reviewed Conference Papers}
\vspace{1em}
\begin{itemize}
\onehalfspacing

\item Humphrey, E. J., Salamon, J. Nieto, O., Forsyth, J., Bittner, R. M., and Bello, J. P. ``JAMS: A JSON Annotated Music Specification for Reproducible MIR Research.''{\it Proceedings of the 15th International Society of Music Information Retrieval (ISMIR)}, Taipei, Taiwan, October 2014.

\item Raffel, C., McFee, B., Humphrey, E. J., Salamon, J. Nieto, O., Liang, D., and Ellis, D. P. W. ``mir eval: A Transparent Implementation of Common MIR Metrics''{\it Proceedings of the 15th International Society of Music Information Retrieval (ISMIR)}, Taipei, Taiwan, October 2014.

\item Humphrey, E. J., Nieto, O., and Bello, J. P. ``Data Driven and Discriminative Projections for Large Scale Cover Song Identification.''{to appear in \it Proceedings of the 14th International Society of Music Information Retrieval (ISMIR)}, Curitiba, Brazil, November 2013.

\item Humphrey, E. J. and Bello, J.P. ``From Music Audio to Guitar Tablature: Teaching Deep Convolutional Networks to Play Guitar.'' {\it Proceedings of the International Conference on Acoustic Signals and Speech Processing (ICASSP)}, Florence, Italy, May 2014.

\item Humphrey, E. J., Nieto, O., and Bello, J. P. ``Data Driven and Discriminative Projections for Large Scale Cover Song Identification.''{to appear in \it Proceedings of the International Society of Music Information Retrieval (ISMIR)}, Curitiba, Brazil, November 2013.

\item Humphrey, E. J. and Bello, J.P., ``Rethinking Automatic Chord Recognition with Convolutional Neural Networks.'' {\it Proceedings of the International Conference on Machine Learning and Applications (ICMLA)}, Boca Raton, FL, December 2012.

\item Humphrey, E. J., Bello, J. P., and LeCun, Y. ``Moving Beyond Feature Design: Deep Architectures and Automatic Feature Learning in Music Informatics.'' {\it Proceedings of the International Society of Music Information Retrieval (ISMIR)}, Porto, Portugal, October 2012.

\item Nieto, O.,  Humphrey, E. J., and Bello, J. P. ``Compressing Music Recordings into Audio Summaries.''{in \it Proceedings of the International Society of Music Information Retrieval (ISMIR)}, Porto, Portugal, October 2012.

\item Humphrey, E. J., Cho, T. and Bello, J.P. ``Learning a Robust Tonnetz-space Transform for Automatic Chord Recognition.'' {\it Proceedings of the International Conference on Acoustic Signals and Speech Processing (ICASSP)}, Kyoto, Japan, March 2012.

\item Humphrey, E. J., Glennon, A. and Bello, J.P., ``Non-Linear Semantic Embedding for Organizing Large Instrument Sample Libraries.'' {\it Proceedings of the International Conference on Machine Learning and Applications (ICMLA)}, Honolulu, HI, December 2011.

\end{itemize}

