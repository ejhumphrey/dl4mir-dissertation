
% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- introduction file header -----------------------
% the code below specifies where the figures are stored
\graphicspath{{3/figures/}}

\chapter{Deep Learning}
\label{chp:background}
% This article is structured very well: http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks
% Goals:
%  - Provide contextual background for deep networks (how did we get here)
%  - Equate traditional digital signal processing and deep learning
%  - Define the necessary pieces of deep learning

% Outline
%  - Background
%   -- Origins
%   -- Breakthroughs
%   -- State of the Art / recent developments
%  - Formal Definitions
%   -- What is ``deep learning''; non-linear function, parameterized, differentiable, optimized to a given objective criterion.
%   -- Architectures
%    --- Affine Layers
%    --- Convolutions (2d, 3d)
%    --- Nonlinearities
%    --- Pooling
%   -- Energy-based Learning
%    --- Scalar objective criteria (Loss Functionals)
%    --- Stochastic Minibatch Gradient Descent
%   -- Tricks
%    --- Regularization & Sparsity
%    --- Parameter Initialization
%    --- Dropout
%    --- Data Augmentation
%  - Relationship to Audio DSP


Deep learning descends from a long and contested history of artificial intelligence, information theory, and computer science.
The goals of the this chapter are two-fold:
Section \ref{background} first offers a concise summary of the history of deep learning in three parts, detailing the origins, critical advances, and current state of the art of neural networks.

Afterwards, a formal treatment of deep learning is addressed:
Section \ref{sec:arch} introduces the architectural components of deep networks;
Section \ref{sec:learning} formally addresses the actual ``learning'' process;
Section \ref{sec:tricks} covers a handful of pertinent tricks of the trade, enabling the practical realization of such models;
and finally, Section \ref{sec:dsp} draws the relationships between this lineage and conventional approaches to digital signal processing.


\section{A Brief History of Neural Networks}

Despite the recent wave of interest and excitement surrounding it, the core principles of deep learning were originally devised halfway through the 20th century, and grounded in mathematics established even earlier.
A direct descendant of the now-infamous neural network, the very mention of deep learning often ellicts several justified questions: What's the difference? What's changed? Why \emph{now}?
Thus, before delving into a formal treatment of the topic, it is worthwhile to contextualize the research trajectory that led to this point.
% For this reason is particularly valuable to consider the lineage of these methods in an effort to understand why it is only now that such approaches have begun to gain popularity and scientific acclaim.


\subsection{Origins (pre-1980)}
\label{subsec:origins}

For Western Europe and those in its sphere of influence, the Age of Enlightenment marked a golden era of human knowledge, consisting of great advances in many diverse fields, such as mathematics, philosophy, and the physical sciences.
Long had humanity contemplated the notions of consciousness and reasoning, but here brilliant thinkers began to return to and explore these concepts with resolve.
From the efforts of scholars like Gottfried Leibnitz, Thomas Hobbes, and George Boole, formal logic blossomed into its own mathematical discipline.
In doing so, it became possible to symbolically express the act of reasoning, whereby rational thought could be described by a system of equations to be transformed or even solved.

It was this critical development ---the idea of logical computation--- that encouraged subsequent generations to speculate on the apparent feasibility of artificial intelligence.
And, coinciding with the advent of electricity in the 20th century, mathematicians, philosophers, and scientists of the modern era sought to create machines that could \emph{think}.
While the space of relevant contributions is too great to enumerate here, there were a handful of breakthroughs that would prove integral to the field of computer science.
In 1936, Alan Turing devised the concept of a ``universal machine'', which would lead to the proof that a system of binary states, e.g. true and false, could be used to perform \emph{any} mathmatical operation \cite{Turing1936}.
Only a year later, Claude Shannon demonstrated in his \emph{master's} thesis that Boolean logic could be implemented in electrical circuits via switches and relays, forming the basis of the modern computer \cite{Shannon1937}.
Shortly thereafter, in 1943, Pitts and McCulloch constructed the first artifical neuron, a simple computational model inspired by discoveries in neuroscience \cite{Pitts1943}.
By coarse analogy to a biology, an artifical neuron ``fires'' when a weighted combination of its inputs eclipses a given threshold, e.g. 0:

% \begin{align*}
% f(x | w) = h( \mathbf{w}^T \dot \mathbf{x}) \\
% h(y) = \left\{
%      \begin{array}{lr}
%        1 : y \ge 0\\
%        0 : y < 0\\
%      \end{array}
%    \right.
% \end{align*}

\begin{align*}
  f(x~|~w) = h(w^T~\cdot~x)\\
  h(y) = \left\{
    \begin{array}{ll}
      1 : y \ge 0\\
      0 : y < 0\\
    \end{array}
  \right.
\label{eq:perceptron}
\end{align*}

\noindent Importantly, as shown in Figure \ref{fig:neuron_logic}, it was demonstrated that such a model could be used to reproduce Boolean operations, such as AND or OR.
Given the clear application to the field of computational logic, artifical neurons only further encouraged the pursuit of artificially ``intelligent'' machines.

% Perceptron!
On its own, an artifical neuron is only a general processing structure, and the parameters it takes will specify the precise behavior of the model.
Arriving at these parameters, however, was nontrivial and required manual derivation.
Thus, in 1957, Frank Rosenblatt's invention of the \emph{Perceptron} algorithm signficantly altered how artificial neurons were conceived \cite{Rosenblatt1957}.
Building upon the work of Pitts and McCulloch, the algorithm, given in \ref{alg:perceptron_fit}, offered an automated method of ``learning'' the parameters necessary to achieve binary classification over a collection of data:

% \begin{algorithm}
% % \caption{Fit a Perceptron to a collection of data.}
% \label{alg:perceptron_fit}
% \begin{algorithmic}[1]
% \Procedure{fit}{$\mathbf{x}, \mathbf{y}, \nu, n_{max}$}
%     \State $\mathbf{w} \gets \mathcal{N}(\mu, \sigma)$
%     \State $n \gets 1$
%     \While $\sum~|e| > 0$ or $n > n_{max}$
%         \State $\mathbf{z} = f(\mathbf{x} | \mathbf{w})$
%         \State $\mathbf{e} = \mathbf{z} - \mathbf{y}$
%         \State $\mathbf{w} \gets \mathbf{w} + \nu (\mathbf{e}^T \dot \mathbf{x})^T$
%         \State $n \gets n + 1$
%     \EndFor
%     \State Return $\mathbf{w}$
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}

\begin{algorithm}[H]
\caption{Compute sum of integers in array}
\label{array-sum}
\begin{algorithmic}[1]
\Procedure{ArraySum}{$A$}
    \State $sum = 0$
    \For {each integer $i$ in $A$}
        \State $sum = sum + i$
    \EndFor
    \State Return $sum$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\noindent First, the weights, $\mathbf{w}$, are set to some initial condition.
Then, in an iterative manner, outputs, $\mathbf{z}$, are predicted from the inputs, $\mathbf{x}$, and the weights are updated with a scaled version of the incorrectly classified inputs.
Note that the error vector, $\mathbf{e}$ is only non-zero where the predicted values are wrong, and thus the algorithm ceases execution when all datapoints are classified correctly, or it reaches some number of iterations.
A visual example of this is given in Figure \ref{fig:linsep}.
Here, a perceptron is used to separate two classes of data, drawn from different Gaussian distributions.
As the algorithm proceeds, the total error decreases until a decision boundary is found that correctly classifies all datapoints.

\begin{figure}
\begin{centering}
\includegraphics[width=0.6\textwidth]{linsep}
\caption{Linearly separable data classified by a trained perceptron.}
\label{fig:linsep}
\end{centering}
\end{figure}

% Promise and limitations
Once implemented in hardware, using a slew of potentiometers, motors, and photocells, Rosenblatt's' ``Mark I Perceptron'' drew considerable attention from the press and the research community alike.
The \emph{New York Times} was quick to publish ambitious claims as to the promise this breakthrough held for artificial intelligence and the speed at which subsequent advances would be realized, much to the chagrin of the AI community \cite{somebody}.
% The Perceptron was not without limitations
In their book, \emph{Perceptrons}, published in 1969, Minsky and Papert demonstrated that the model is rather limited in the functions it can reproduce.
As such, perceptrons could only be used to classify linearly separable conditions, and not more complex operations, such as the exclusive-or (XOR).
This condition is illustrated in Figure \ref{fig:mlp_ftw}, where no single line can be draw to correctly classify the data.

% Multilayer Perceptrons
This was a critical limitation for researchers in the field of computational logic; if a perceptron could not perform a simple exclusive-or, how could a machine be expected to reason?
The answer, as it would turn out, could be found by transforming how the XOR is expressed symbolically.
Rearranging terms, an equivalent function can be rewritten as the disjuction (OR) of two complementary conjuctions (AND):

\begin{equation}
\label{eq:xor}
p \oplus q = (p \wedge \neg q) \vee (\neg p \wedge q)
\end{equation}

\noindent While it is true that a single Perceptron cannot achieve this operation directly, a combination of \emph{three} can: two are used to perform each AND operation and corresponding negation, while a third performs the OR operation.
Considering the scenario in Figure \ref{fig:mlp_ftw}, this condition can now be easily separated by a \emph{multilayer} perceptron (MLP).
Therefore, arbitrarily complex functions could be obtained by cascading simpler non-linear operations, in what would be commonly reffered to as a \emph{neural network}.
In fact, it would later be shown by the \emph{universal approximation theorem} that a neural network is actually able to model \emph{any} continous function within some tolerance \cite{Cybenko1989, Hornik1991}.

\begin{figure}
\begin{centering}
\includegraphics[width=0.6\textwidth]{mlp_ftw}
\caption{Demonstration of the decision boundaries for a \emph{multi-layer} perceptron.}
\label{fig:mlp_ftw}
\end{centering}
\end{figure}

% Other problems
Despite this observation, the study of neural networks languished through the closing decades of the 20th century, suffering a considerable downtick of interest and funding support.
While the representational power of multilayer perceptrons was recognized early on, many often claimed ---incorrectly--- that they could not be used to solve complex problems.
More practically, those who continued to pursue neural network research were faced with an array of other challenges.
Neural networks were prone to over-fitting, extremely slow to compute weight updates over a large number of datapoints, and difficult to train with more than a few layers.
% Ahead of the times
% Theory outpaced technological capacity. Over-hyped
In isolation, these issues might merely have a posed a standard research obstacle, but coupled with the failed promises of overzealous researchers, such difficulties would malign neural network research for some time.
% This resulted in what is affectionately known as one of the ``AI winter'', things got bad.


\subsection{Scientific Milestones (1980--2010)}
\label{sec:advances}

Despite widespread pessimism toward neural computing, a select few would persevere through this ``AI Winter''.
Over the course of thirty odd years, these diligent efforts would yield a array of crucial breakthroughs and fundamentally change the landscape of neural network research.
While no single contribution can truly be credited with reviving the field, each would play an integral role in helping the research community warm up to neural networks.

\subsubsection{Stochastic Gradient Descent}
\label{subsec:sgd}

% Back-propagation of errors
Via the perceptron algorithm, a machine effectively ``learns'' to classify data by finding parameters that optimize a given objective function.
Originally this was framed as a minimization, as in Eq. \ref{eq:perceptron}, such that the error, known as the ``perceptron loss'', could be incrementally decreased by taking small steps toward better parameters.
This optimization strategy must be modified in the multilayer case, as it is not possible to directly update the parameters through the heaviside, or unit step, function.
Early researchers noted that if the discontinuous activation function is replaced with a logistic, or sigmoid, the composite multilayer network is \emph{differentiable}.
The derivative of each coefficient, regardless of layer, can be computed with respect to the loss, thus \emph{back-propagating} error through the network \cite{Hinton1986}.

% Issues of gradient descent
Error back-propagation demonstrated that, so long as a network was everywhere differentiable, it was theoretically possible to optimize the objective function over a dataset by slowly adjusting parameters towards better solutions.
This optimization method, referred to as \emph{gradient descent}, was not without its deficiencies, however.
First and foremost, the loss of a given configuration was typically computed over the entire dataset, which may prove computationally expensive for sufficiently large collections.
Additionally, gradient descent is a greedy search algorithm, and was reportedly prone to finding poor local minima, as illustrated in Figure \ref{fig:local_min}.

%Stochastic Gradient Descent
In 1987, however, LeCun demonstrated that a \emph{stochastic} variant of gradient descent could potentially be used to circumvent these issues \cite{LeCun1987}.
Whereas conventional gradient descent had been formulated as an optimization over an entirely collection of datapoints, this new version randomly sampled training samples from a collection and computed an update step based on single datapoint.
Though the gradient is much noiser, this strategy is still effective,significantly faster, and may even be less susceptible to poor local minima as a result.
Importantly, this notion can be generalized as \emph{mini-batch} gradient descent, where the number of datapoints used to compute parameter updates is a hyperparameters, varying from 1 to the total number of datapoints available for training.
Covarying inversely with the learning rate, illustrated in Figure \ref{fig:sgd}, larger batch sizes serve to smooth parameter updates as a function of iteration.

\subsubsection{Convolutional Neural Networks}
\label{subsec:convnets}

% Fully connected networks, bleh; sensitive to translation and scale
% TODO: Needs a better context sentence

Though equal parts remarkable and encouraging, the universal approximation theorem says nothing about how one might parameterize a neural network to achieve some desired behavior.
Thus, despite such overwhelming promise, early neural networks often resulted in poor approximations of the functions being modeled, because the flexibility of the model vastly surpassed the amount of data available to train it.
This flexibility was apparent in computer vision tasks, as multilayer perceptrons struggled to cope with simple homographic deformations, such as translation, scaling, or rotation.

% Visual cortex, neocognitron
Natural vision systems, on the other hand, are quite robust to these kinds of variation, and, as before, researchers looked to biology for inspiration in modeling these processes.
One particularly crucial study was that of Hubel and Wiesel in 1959, who experimented on the neural behavior of the feline visual cortex \cite{}.
By presenting visual stimuli while probing the brain with electrodes, they discovered two different kinds of neurons ---simple cells and complex cells--- organized hierarchically.
Experimental results indicated that the former is responsible for local feature extraction, while the latter combines the inputs of simple cells to absorb small amounts of variation in the observed stimuli.

% ConvNets, LeNet5, handwritten digits
These ideas were first incorporated in the Neocognitron of \cite{Fukushima1988}, and realized successfully as a convolutional neural network, or ``ConvNet'', for shape recognition and handwritten digit classification \cite{LeCun1990, LeCun1998}.
Importantly, ConvNets incorporated three key design features.
\emph{Local receptive fields}, where compute features from a small neighborhood of pixels, exploiting the observation that nearby pixel values tend to be highly correlated.
Smaller receptive fields require fewer parameters, thus reducing the overall size of the model and the amount of data needed to train it.
Applying local receptive fields as a convolution results in effectively sharing the weights over all positions in an image.
Thus \emph{weight sharing} reduces the number of parameters even further, while allowing the network to identify features regardless of position in an image.
Finally, \emph{max-pooling} reduces small neighborhoods to the largest value within it, introducing not only shift but a small amount of scale and rotation invariance.
Taken together, these advantageous attributes directly resolved many of the issues plaguing multilayer perceptrons, and proved to be the most successful instance of neural computing for nearly two decades.


% Theory preceded data
\subsubsection{Proliferation of Digital Data}
\label{subsec:perceptrons}

% Proliferation of digital content
During the first rise and fall of neural networks, digital data was far more the exception than the rule.
Digitized audio and images didn't become common until the end of the 20th century.
Even so, the data rate of these signals made processing them computationally prohibitive: ata
The internet, conceived in the 1960s, didn't rise to public prominence in the US until the 1990s.

% Labeled datasets grew, intentionally or otherwise
Researchers had time to build large labeled datasets.
Along with the internet, it became possible to use weakly labeled data or distribute the annotation effort.
MNIST is the hallmark of curated datasets.
TIMIT for speech
CAPTCHA for optical character recognition.
ImageNet, GWAP.
Weakly labeled data from the internet, researchers got clever.

% unlabeled data has uses too
Finally in 2006, one such method, called contrastive divergence was proven successful, using a slightly different processing model known as a Restricted Boltzmann machine (RBM).
The important characteristic of an RBM is that it can be used to generate, as well as process, information in both directions.
This is fundamental to contrastive divergence, which trains each layer of a deep network separately by learning to recreate its input data.
By analogy, this approach aims to discover structure in the data that occurs with high probability and effectively ``ignore'' everything else.
When strung together as a full network, these systems are referred to as deep belief networks (DBNs), as a result of their probabilistic formulation.
Subsequently, a similar model called an autoencoder \cite{Vincent2010}, or ---an alternative interpretation from another research team--- Predictive Sparse Decomposition \cite{Ranzato2007}, incorporates roughly the same principles with deterministic interpretation of these concepts.
In either view of the parameter optimization problem, learning algorithms now exist that can leverage large quantities of unlabeled data to tune large networks with minimal supervision.
These advances have collectively ushered in a new era of machine learning, referred to as \emph{deep learning}, named so for the multiple levels of information processing and the manner in which the parameters of the system are discovered from data.


% Theory preceded technology
\subsubsection{Advances in Computing Technology}
\label{subsubsec:hardware}

% Neural networks were devised in the 60s; in the 80s, computers sucked
While enough cannot be said about the scientific contributions of neural network researchers over the last 40 years, it is critical to appreciate how computing technology has evolved over that time span.
In the days of Rosenblatt and Minsky, computers consisted of transistors that were visible to the naked eye, filled entire rooms, and cost a small fortune.
More recently, personal computers of the 1980s had kilobytes of memory, central processing units (CPUs) operated in the range of tens of megahertz, and were still far too expensive for all but the elite or dedicate.
Computation was still quite slow as a result, and thus the process of traning neural networks took an impressive amount of time.
To combat these difficulties, researchers often attempted to cut corners by using smaller models, smaller datasets, and training with fewer iterations.
Somewhat unsurprisingly, the common experience with neural networks was quite unfavorable.

% Now better; bigger hard disks, more memory, faster processors, GPUs, async SGD, distributed computing, better tools (Theano, Torch)
Eventually, though, technology could catch up to the theory.
Hardware became increasingly smaller, memory grew to the size of gigabytes, processors accelerated for a time without bound faster, and the cost of computers dropped to the point of near ubiquity.
On the heels of these developments, other hardware and tools were adapted to facilitate neural network research, such as parallel computing with graphics processing units (GPUs) and accessible software tools, e.g. Theano\footnote{} or Torch\footnote{}.
Combined, singificantly better technology directly enabled research at unprecedented scale, accelerating progress and relaxing computational constraints imposed by technical limitations.

% Additionally, significant increases in computational power, and especially advances in parallel computing via GPUs, make deep learning not only feasible, but in some cases an efficient approach to research.
% Evidenced by the recent work of \cite{kittehs}, some are beginning to attempt large scale deep learning on unprecedented quantities of data.
% Similarly, software libraries and toolkits \cite{Theano, Torch, Caafe} are now available that leverage these computational gains to make deep learning more accessible to the entire research community.


\subsection{Modern Rennaissance (post-2010)}
\label{subsec:rennaissance}

Industrial strength; Google, Facebook, Baidu, Microsoft
Press; YouTube and Cats, Yann LeCun, Geoff Hinton, IEEE, NYTimes
Competitions: Kaggle, Netflix, ImageNet


\section{Core Concepts}
\label{sec:example}

% The "what" of deep learning
At the highest level, deep learning is an approach to nonlinear system design that combines high level function design with numerical optimzation in an effort to model some observable behavior.


Having covered the ``why'' of deep learning, this section aims to generalize the concepts of classic neural computation into a more general presentation of its fundamental components.



\subsection{Modular Architectures}
\label{subsec:architectures}

S'all fair game so long as you can differentiate it.

\subsubsection{Linear Algebra}

% Fully connected / affine
Conventionally, a neural network transforms an input $X_{1}$ into an output $Z_{L}$ via a composite nonlinear function $F(\cdot \vert \Theta)$, given a parameter set $\Theta$.
This is traditionally achieved as a cascade of $L$ simpler nonlinear functions $f_l(\cdot \vert \theta_l)$, referred to as layers, the order of which is indexed by $l$:

\begin{equation}
\label{eq:layers}
F(X_{1} \vert \Theta) = f_{L}(  ... f_2(f_1(X_{1} \vert \theta_1) \vert \theta_2) ) ... \vert \theta_{L})
\end{equation}

\noindent In this formulation, $F = [f_1, f_2, ... f_{L} ]$ is the set of layer functions, $\Theta = [\theta_1, \theta_2, ... \theta_{L} ]$ is the corresponding set of layer parameters, the output of one layer is passed as the input to the next, as $X_{l+1} = Z_{l}$, and the overall \emph{depth} of the network is given by $L$.


While a layer $f_l$ can be any differentiable function, we limit our focus in this work to fully-connected, or \emph{affine}, transformations, defined by the following:

\begin{equation}
\label{eq:fclayer}
f_l(X_l \vert \theta_l) = h( W \bullet X_{l} + b), \theta_l = [W, b]
\end{equation}

\noindent Here, the input $X_l$ is flattened to a column vector of length $N$ and the dot-product is computed with a weight matrix $W$ of shape $(M, N)$, followed by an additive vector bias term $b$ with length $M$.
Note that an affine layer transforms an $N$-dimensional input to an $M$-dimensional output, referred to as the \emph{width} of the layer.
The final operation is a point-wise nonlinearity, $h(\cdot)$, defined here as $tanh(\cdot)$, which is bounded on $(-1, 1)$.

% Local Receptive Fields
Fully connected matrices that are only connected to local neighborhoods.
This can be generalized as a very large dot product where most parameters are fixed to zero.

% Convolutions
Same formulation as local receptive fields, except the parameters are tied across location.

% Other connectivities
Could be data driven, a la Coates et al \cite{Coates2012?}.

\subsubsection{Nonlinearities}

Logistic / sigmoid, hyperbolic tangent.
Close cousins, but generally hyperbolic tangent has gained favor for being centered about the origin.

Rectified linear, piecewise linear model. % relationship to random forests?
Results in naturally sparse representations.
Doesn't saturate in the positve region, but no information flows in the negative mode.
Leaky ReLU, gradient everywhere.

%Softmax
When used as a classification system, the first $L-1$ layers of a neural network can be viewed as feature extractors, and the last layer, $f_L$, is a simple linear classifier.
The output representation can be made to behave like a posterior by constraining the $L_1$-norm of the output to equal $1$.
Traditionally, the probability mass function $P(\cdot)$ for an input $X_1$ is achieved by applying the softmax operation to the output of the network, $Z_L$, defined as follows:

% TODO: Isn't there a Beta term here?
\begin{equation}
\label{eq:softmax}
\sigma(Z) = \frac{\exp(\beta Z)}{ \sum_{k=1}^{K}\exp{(\beta Z_k)}}
\end{equation}

\noindent Again, note that $Z$ is the output of the final layer, $f_L$, and thus $K$ is the dimensionality of the classifier and equal to the number of classes in the problem.
Therefore, the most likely class for a prediction $Z$, is given by $k = argmax(\sigma(Z))$.

Notably, any differentiable nonlinearity could conceivably be used here.


\subsubsection{Pooling}

Max pooling is the most common form, usually applied to a local neighborhood.
Also L2 variant
Relationship to max-out networks.


\subsection{Energy-based Learning}

Ideally, these features are \emph{learned} automatically using what are known as gradient methods, the basic idea being that a function attempts to reduce how ``wrong'' it is by taking small steps toward better answers; by analogy, this is a bit like climbing down a mountain blind-folded.
For this to be possible, the activation of each node, as expressed in Equation (\ref{eq:perceptron}), must be softened such that the function is differentiable.


Having written the full network as a probability mass function, the network can be trained by iteratively minimizing the negative log-likelihood of the correct class for a set of $K$ observations:

\begin{equation}
\label{eq:nll}
\mathcal{L}=-\sum_{k=0}^K log(P(X^k = Y^k \vert \Theta))
\end{equation}

\noindent Expressed in this manner, $X^k$ and $Y^k$ are the input data and corresponding class label, respectively of the $k^{th}$ observation.

We can then minimize this loss function via mini-batch stochastic gradient descent.
Here, gradients are computed with $K>1$, but much smaller than the total number of observations, by sampling datapoints from the training set and averaging the loss over the batch.
Specifically, the update rule for $\Theta$ is defined as its difference with the gradient of the scalar loss $\mathcal{L}$ with respect to the parameters $\Theta$, weighted by the learning rate $\eta$, given by the following:

\begin{equation}
\label{eq:updaterule}
\Theta \leftarrow \Theta - \eta * \frac{ \delta \mathcal{L}}{\delta \Theta}
\end{equation}


\subsection{Tricks of the Trade}
\label{subsec:tricks}

%   -- Tricks
\subsubsection{Regularization \& Sparsity}
Reduce over-fitting via weight decay.
L2 penalties make values small without going to zero

Sparse representation are good.
L0 and L1 loss are closely related, will drive values all the way to zero.

\subsubsection{Data-Driven Initialization}

Core idea behind the last significant breakthrough of the modern era.
Get parameters of the network closer to a good solution.
Resolves poorly conditioned networks.
Alleviates the vanishing gradient problem.

Recently it was observed that with enough data, rectified linear units, or both, this is less crucial than once thought.


\subsubsection{Dropout}
Preventing co-adaptation of parameters.
Related to model averaging.
Reduces theoretical bounds on generalization error.


\subsubsection{Data Augmentation}


\subsubsection{Bootstrapping Additional Information}
Sparse coding
Jordan Net


% Looking closely at recent work in deep learning, most of the effort has been invested in areas of static input representations, i.e. computer vision and still images.
% As evidenced by music, audio, and video, time plays an extremely important role in the way that information is conveyed.
% Though CNNs have been applied to signals with a temporal dimension \cite{LeCun1994}, recursive architectures have shown promise for addressing the problem of time and sequences in data.
% Initial efforts to train RNNs however, even with supervised methods, proved extremely challenging, with the observed phenomena manifesting as numerical stability issues of the gradient.
% Due to the recursive nature of the model, backpropagating the error signal through time is prone to make the amplitude of the gradient either vanish or explode.
% Despite these difficulties, several approaches and optimization methods have been developed in recent years that encourage further exploration of these models.

% One of the earliest successful attempts to circumvent the issue of gradient stability was realized through a method called LSTM \cite{Schmidhuber1997}, where the gradient signal is ``latched'' by a hidden unit such that its information could be saved long enough to be useful.
% Similar notions surround the notion of reservoir computing, such as echo-state networks, which employ feedback connections to cache arbitrary temporal information such that a second non-recursive layer ---being far easier to train--- might be able to make sense of it \cite{Jaeger2002}.
% Alternatively, conditional DBNs have been employed to model, classify, and generate convincing sequences of human motion \cite{JMLR2011}, and can be thought of as a probabilistic CNN.
% Recent research has found that more complex optimization methods are sufficient to train RNNs directly without changing the underlying model \cite{Martens2010}, giving encouraging qualitative results on motion \cite{Sutskever2008} and text generation \cite{Sutskever2011}.
% There has also been initial work investigating discriminative recursive models that also leverage unsupervised training \cite{Rolfe2013}.
% This research is particularly exciting, as it demonstrates that such models are actually within reach.
% Reflecting on the corpus of deep learning work in the past decade however, it should be noted that the majority of these breakthroughs ignore audio, and even more so music, signals, focusing instead on vision or natural language processing tasks.


\section{Previous Efforts in Automatic Music Description}

This realization is beginning to spread throughout the MIR community, and there is, understandably, an increasing interest in the application of deep learning methods to these problems.
Among the first uses of deep learning were the application of CNNs to the detection of onsets \cite{Lacoste2007} and long short-term memory to model symbolic music sequences \cite{Eck2008}.
Deep Belief Networks quickly gained popularity for genre recognition \cite{Hamel2009}, mood estimation \cite{Schmidt2011}, note transcription \cite{Nam2011}, and artist recognition \cite{Dieleman2011}, but somewhat surprisingly many of these methods operate on single, short-time observations of the music signal.
Incorporating longer time-scales, CNNs have also been used to classify inputs on the order of seconds for, again, genre recognition \cite{Li2010}, instrument classification \cite{Humphrey2010} and chord estimation \cite{Humphrey2011, Humphrey2012b}.
Predictive Sparse Decomposition (PSD) and methods inspired by sparse coding have also seen a spike in activity over the last two years for a variety of tasks \cite{Henaff2011, Nam2012}, but despite being developed for deep learning, neither of these systems are actually hierarchical.

% It is worthwhile to note that many, if not all, of these works have achieved state of the art performance on their respective tasks, often in the first application of the method to the area.
% Any instances where this has occurred, however, deep learning techniques have simply been directly some kind of derived audio representation with minimal modification.
% It is therefore important to realize that these methods have never been tailored to address the nuances of music signals, and more importantly time.
% Overall, only CNNs make an effort to consider observations on the order of seconds, but do so in an admittedly awkward way, requiring the observation length be defined \emph{a priori}.
% The machine is ultimately limited to the amount of information it sees in its analysis, and there is obviously no fixed duration in which all musical behavior resides.

Due to advances in machine computation, there are primarily two different ways to conceptualize the design of information processing models.
One approach is based on signal theory as viewed through the lens of electrical engineering, whereas the other focuses on graphical models of artificial neural processing grounded in computer science.
Though broad topics of study in themselves, they are worth mentioning together for two crucial reasons: first, a majority of inertia developed within the field of MIR is due largely to its roots in the former; and second, it is a useful juxtaposition to form when marrying these disconnected topics.

%Electrical engineering formalized as a discipline in the middle of the $19^{th}$ century on the heels of inventions such as the telegraph and electric power distribution.

Much early work in electrical engineering stemmed from the realization that electricity could be used to represent measurable phenomena and therefore capture, store, and transmit this information.
These representations are referred to as analog signals because they use one representation ---time-varying voltage, for example--- to mirror another one ---such as a sound pressure wave--- in a continuous fashion.
It was soon discovered that not only could this information be represented via electricity, but that it was also possible to analyze and manipulate it for a variety of applications.
Heavily grounded in classic numerical analysis methods of the $17^{th}$ and $18^{th}$ centuries, these methods are referred to as signal processing.
Through experimentation in physics, scientists found that certain electromagnetic materials could be used to affect electricity in predictable ways, and when connected in an electrical circuit could be used to filter, or augment, signals.
Importantly, the need to solve signal processing systems by inspection requires that the entire system be linear and time invariant (LTI), because the violation of either property makes mathematical analysis far too difficult, if not altogether impossible.

With the advent of solid-state transistors and subsequently computers in the mid-$20^{th}$, digital representations of signals could be produced by sampling analog waveforms to be manipulated numerically, referred to as digital signal processing (DSP).
Digital signals---those that also represent some kind of real world phenomena, at least---differ from analog ones in two important aspects: the signal takes values from a finite set of discrete numbers, and these quantities are measured at precise moments in time.
Therefore, digital signals are specified by a sequence of real-valued numbers $x[n]$, rather than a generated, continuous function $x(t)$.
When this transition to numerical computation began, the effort was made to reformulate previous knowledge regarding analog signal theory and processing into the digital domain.
Similar to those in analog theory, digital filters can be expressed explicitly by a transfer function, which can be translated directly to a set of coefficients representing the linear difference equation, generically given by the following:

\begin{equation}
\label{eq:diffeq}
\begin{array}{rcr}
y[n] & = & b_0x[n] + b_1x[n-1] + b_2x[n-2] + b_3x[n-3] \ldots \\
 & & - a_1y[n-1] - a_2y[n-2] - a_3y[n-3] \ldots
\end{array}
\end{equation}

There are typically said to be two categories of digital filters: Finite Impulse Response (FIR), or non-recursive, filters and Infinite Impulse Response (IIR), or recursive, filters.
As illustrated in Figure \ref{fig:filters}, an FIR filter is actually a special case of the more general IIR filter, where the feedback connections---the $a$-coefficients in Equation (\ref{eq:diffeq})---have all been set to zero.
Though digital filters are not constrained by the availability or precision of manufactured electrical components, they are still traditionally constrained to be LTI systems for the same reasons as their analog counterparts.
As a result, non-linear signal processing models in both the analog and digital domains are historically unpopular due to inherent challenges in the mathematical formulation.

\begin{figure}[!t]
\centering
\includegraphics[width=\textwidth]{filters_2}
\caption{\small{Finite Impulse Response (i) and Infinite Impulse Response (ii) digital filters. Multiplication is shown along the edges of each connection.}}
\label{fig:filters}
\end{figure}


\section{Summary}
\label{sec:summary}

% Origins
Building upon formal logic and biological analogy, neural networks were devised in the 1960s as an approach to information processing.
However, after inital promise, they were largely met with skepticism, indifference, or worse through the remainder of the century.
% What changed?
The few who perservered made various discoveries that, coupled with steady advances in computing technology, would eventually return neural computation to the fore:
mini-batch stochastic gradient descent made optimization computationally efficient and less susceptible to poor local minima, and encouraged further exploration of numerical optimization methods;
convolutional networks reduced model complexity and over-fitting through scale and translation invariance;
and lastly, larger labeled datasets reduced overfitting, and abundant unlabeled data could be used to better initialize networks in an unsupervised manner.

Deep learning is therefore based on two principles: first, that complex problems can be decomposed into a series of simpler subproblems; and second, what exactly these simpler subproblems are or should be can be discovered from a corpus of data.
