
% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- introduction file header -----------------------
% the code below specifies where the figures are stored
\graphicspath{{7/figures/}}

\chapter{Workflows for Reproducible Research}
\label{chp:reproducibility}

In recent years, the philosophy of open source software has begun taking root in scientific research, particularly among computer science efforts.
There are several reasons why open research is beneficial to the greater body of human knowledge, but three are of particular value here.
First, sharing code and data allows others to reproduce previous results, a fundamental tenet of the scientific method.
Open source implementations are invaluable for sufficiently complex systems.
It may be near impossible to describe every minute detail in a publication necessary for someone else to replicate the obtained results; for some cases, in fact, the only artifact that can do this unambiguously is the source code itself.
Second, and in a related vein, open research makes is both easier and faster to build upon and extend the previous work of others.
Even in the instance a researcher is able to recreate a published system, the time and effort necessary to get to this point is significant and arguably unnecessary.
Granted, while there is an educational component inherent to the re-implementation of previous work, the situation is akin to long division: it is certainly valuable to learn \emph{how} to divide by hand, but no one shuns the use of a calculator on a day to day basis.
Lastly, it is a good and responsible act to contribute tools and software back to the larger research community.
All research stands on the shoulders of previous efforts, from improving on a recently published algorithm to the decades-old linear algebra routines doing all its number crunching.
The reality is that no one individual has ever truly solved anything on their own, and sharing the fruits of one's research endeavors serves the common good.

With these motivations in mind, this chapter details the various open source contributions made in the course of this work, culminating in a single code repository used to produce the results contained herein.
These software tools consist of the following:
Section \ref{sec:jams} describes JAMS, a JSON annotated music specification;
Section \ref{sec:biggie} introduces Biggie, an HDF5 interface for interacting with notoriously big data;
Section \ref{sec:optimus} details a user-friendly library for building and serializing arbitrary acyclic processing graphs;
Section \ref{sec:mir_eval} discusses relevant contributions to \texttt{mir\_eval}, a transparent framework for evaluating MIR systems;
and finally, Section \ref{sec:dl4mir} outlines the framework used here to complete this research.


\section{JAMS: A JSON Annotated Music Specification}
\label{sec:jams}

Music annotations ---the collection of observations made by one or more agents about an acoustic music signal--- are an integral component of content-based music informatics methodology, and are necessary for designing, evaluating, and comparing computational systems.
For clarity, the scope of an annotation is constrained to time scales at or below the level of a complete song, such as semantic descriptors (tags) or time-aligned chords labels.
Traditionally, the community has relied on plain text and custom conventions to serialize this data to a file for the purposes of storage and dissemination, collectively referred to as ``lab-files''.
Despite a lack of formal standards, lab-files have been, and continue to be, the preferred file format for a variety of music description tasks, such as beat or onset estimation, chord estimation, or segmentation.

Meanwhile, the interests and requirements of the community are continually evolving, thus testing the practical limitations of lab-files.
Reflecting on this tradition, there are three unfolding research trends that are demanding more of a given annotation format:

\begin{itemize}
\item \textbf{Comprehensive annotation data}:
Rich annotations, like the Billboard dataset\cite{Burgoyne2011expert}, require new, content-specific conventions, increasing the complexity of the software necessary to decode it and the burden on the researcher to use it; such annotations can be so complex, in fact, it becomes necessary to document how to understand and parse the format\cite{De2012parsing}.

\item \textbf{Multiple annotations for a given task}:
The experience of music can be highly subjective, at which point the notion of ``ground truth'' becomes tenuous.
Recent work in automatic chord estimation, both here and in \cite{ni2013understanding}, has shown that multiple reference annotations should be embraced, as they can provide important insight into both system evaluation and the problem formulation itself.

\item \textbf{Multiple concepts for a given signal}:
Although systems are classically developed to describe observations in a single namespace, e.g. chords, there is ongoing discussion toward integrating information across various musical concepts \cite{vincent2010roadmap}.
This has already yielded measurable benefits for the joint estimation of chords and downbeats \cite{papadopoulos2011joint} or chords and segments \cite{mauch2009using}, where leveraging multiple information sources for the same input signal can lead to improved performance.
\end{itemize}


\noindent It has long been acknowledged that lab-files cannot be used to these ends, and various formats and technologies have been previously proposed to alleviate these issues, such as RDF \cite{cannam2006sonic}, HDF5\cite{bertin2011million}, or XML \cite{mckay2005ace}.
However, none of these formats have been widely embraced by the community.
Considering this options, the weak adoption of alternative formats is likely due to the combination of multiple factors.
For example, new tools can be difficult, if not impossible, to integrate into a research workflow because of compatibility issues with a preferred development platform or programming environment.
Additionally, it is a common criticism that the syntax or data model of these alternative formats is non-obvious, verbose, or otherwise confusing.
This is especially problematic when researchers must handle format conversions.
Taken together, the apparent benefits to conversion are outweighed by the tangible costs.


\subsection{Core Design Principles}
\label{subsec:design}

In order to craft an annotation format that might serve the community into the foreseeable future, it is worthwhile to consolidate the lessons learned from both the relative success of lab-files and the challenges faced by alternative formats into a set of principles that might guide the design of a new format.
With this in mind, it is argued that usability, and thus the likelihood of adoption, is a function of three criteria:

\subsubsection{Simplicity}
\label{subsubsec:simplicity}
The value of simplicity is demonstrated by lab-files in two specific ways.
First, the contents are represented in a format that is intuitive, such that the document model clearly matches the data structure and is human-readable, i.e. uses a lightweight syntax.
This is a particular criticism of RDF and XML, which can be verbose compared to plain text.
Second, lab-files are conceptually easy to incorporate into research workflows.
The choice of an alternative file format can be a significant hurdle if it is not widely supported, as is the case with RDF, or the data model of the document does not match the data model of the programming language, as with XML.

\subsubsection{Structure}
\label{subsubsec:structure}
It is important to recognize that lab-files developed as a way to serialize tabular data (i.e. arrays) in a language-independent manner.
Though lab-files excel at this particular use case, they lack the structure required to encode complex data such as hierarchies or mix different data types, such as scalars, strings, multidimensional arrays, etc.
This is a known limitation, and the community has devised a variety of ad hoc strategies to cope with it: folder trees and naming conventions, such as ``\{X\}/\{Y\}/\{Z\}.lab'', where X, Y, and Z correspond to ``artist'', ``album'', and ``title'', respectively\footnote{\url{http://www.isophonics.net/content/reference-annotations}}; parsing rules, such as ``lines beginning with `\#' are to be ignored as comments''; auxiliary websites or articles, decoupled from the annotations themselves, to provide critical information such as syntax, conventions, or methodology.
Alternative representations are able to manage more complex data via standardized markup and named entities, such as fields in the case of RDF or JSON, or IDs, attributes and tags for XML.

\subsubsection{Sustainability}
\label{subsubsec:sustainability}

Recently in MIR, a more concerted effort has been made toward sustainable research methods, which we see positively impacting annotations in two ways.
First, there is considerable value to encoding methodology and metadata directly in an annotation, as doing so makes it easier to both support and maintain the annotation while also enabling direct analyses of this additional information.
Additionally, it is unnecessary for the MIR community to develop every tool and utility ourselves; we should instead leverage well-supported technologies from larger communities when possible.

\subsection{The JAMS Schema}
\label{subsec:schema}

The primary record is an annotation.
Core element is an observation.
time, duration, value, confidence, namespace.
Sparse data, dense data, repeated.
Namespaces are key.
They allow for the filtering and semantic interpretation of the data in an annotation.

Annotations have metadata.
Metadata objects allow for rich descriptions of \emph{who} made the observations.

Annotations are repeated to form a JAMS file.
JAMS files have a top-level file metadata object.
This sponges absolute information about the audio file to which the annotations correspond.
Duration, file identifiers, and such.


\subsection{Python API}
\label{subsec:jams_tools}

Python library: \url{http://github.com/marl/jams}
Allows a simple interface for acting on these files.
Also preconverted many common datasets to this format.
Makes it easier to use other tools to augment audio and the corresponding annotations in parallel \url{http://github.com/bmcfee/muda}.

Example of using JAMS v2.


\section{Biggie}
\label{sec:biggie}

Audio signals are high dimensional data, and it is useful to develop ways of capturing signal level information and associating it all with the same conceptual object, i.e. a recording.
In this way, \emph{biggie} is the high-dimensional, signal-level equivalent to JAMS.
Allows the user to compute an arbitrary set of time-aligned representations from a given audio signal and sponge this information together.
This data is then associated in a key-value store, where each object is given a unique key, making it easy to align signal descriptions with human annotations.
Biggie also allows random access, facilitating bootstrap sampling and stochastic training (see pescador).
Being based on HDF5, biggie scales to arbitrarily large datasets; however, as it shares a common interface with common dictionaries, the memory footprint scales gracefully with available computational resources.


\subsection{Entities}
\label{subsec:entities}

Carrot cake marzipan gummies croissant oat cake pie candy canes chocolate.
Sugar plum jelly beans oat cake cake jujubes jelly chupa chups biscuit \ref{fig:muffin}.
Croissant cotton candy chupa chups.
Cheesecake tart bear claw brownie sugar plum.

\subsection{Stash}
\label{subsec:stash}

Carrot cake marzipan gummies croissant oat cake pie candy canes chocolate.
Sugar plum jelly beans oat cake cake jujubes jelly chupa chups biscuit \ref{fig:muffin}.
Croissant cotton candy chupa chups.
Cheesecake tart bear claw brownie sugar plum.



\subsection{Python API}
\label{subsec:biggie_tools}

\url{http://github.com/ejhumphrey/biggie}.

\section{Optimus}
\label{sec:optimus}

Carrot cake marzipan gummies croissant oat cake pie candy canes chocolate.
Sugar plum jelly beans oat cake cake jujubes jelly chupa chups biscuit \ref{fig:muffin}.
Croissant cotton candy chupa chups.
Cheesecake tart bear claw brownie sugar plum.

\subsection{Goals}
\label{subsec:goals}

Carrot cake marzipan gummies croissant oat cake pie candy canes chocolate.
Sugar plum jelly beans oat cake cake jujubes jelly chupa chups biscuit \ref{fig:muffin}.
Croissant cotton candy chupa chups.
Cheesecake tart bear claw brownie sugar plum.

\subsection{Example}
\label{subsec:example}

Carrot cake marzipan gummies croissant oat cake pie candy canes chocolate.
Sugar plum jelly beans oat cake cake jujubes jelly chupa chups biscuit \ref{fig:muffin}.
Croissant cotton candy chupa chups.
Cheesecake tart bear claw brownie sugar plum.

\url{http://github.com/ejhumphrey/optimus}.


\section{mir\_eval}
\label{sec:mir_eval}

Much of the research in Music Information Retrieval (MIR) involves the development of systems that process raw music data to produce semantic information.
The goal of these systems is frequently defined as attempting to duplicate the
performance of a human listener given the same task~\cite{downie2003toward}.
A natural way to determine a system's effectiveness might be for a human to study the output produced by the system and judge its correctness.
However, this would yield only subjective ratings, and would also be extremely time-consuming when evaluating a system's output over a large corpus of music.

Instead, objective metrics are developed to provide a well-defined way of computing a score which indicates each system's output's correctness.
These metrics typically involve a heuristically-motivated comparison of the system's output to a reference which is known to be correct.
Over time, certain metrics have become standard for each task, so that the performance
of systems created by different researchers can be compared when they are evaluated
over the same dataset~\cite{downie2003toward}.
Unfortunately, this comparison can be confounded by small details of the implementations or procedures that can have disproportionate impacts on the resulting scores.

% MIREX is the standard, but it's not community-developed, not transparent, not well-documented, uses multiple languages, is coupled to NEMA, not easy to use, dependencies, not used outside of MIREX, etc.
For the past 10 years, the yearly Music Information Retrieval Evaluation eXchange
(MIREX) has been a forum for comparing MIR algorithms over common datasets~\cite{downie2008music}.
By providing a standardized shared-task setting, MIREX has become critically useful for tracking progress in MIR research.
MIREX is built upon the Networked Environment for Music Analysis (NEMA)~\cite{west2010networked}, a large-scale system which includes exhaustive functionality for evaluating, summarizing, and displaying evaluation results.
The NEMA codebase includes multiple programming languages and dependencies (some of which, Matlab, are proprietary) so compiling and running it at individual sites is nontrivial.
%
%Due to its scale, characteristics, and intended use,
In consequence, the NEMA system is rarely used for evaluating MIR algorithms outside of
the setting of MIREX~\cite{downie2008music}.
Instead, researchers often create their own implementations of common metrics for evaluating their algorithms.
These implementations are thus not standardized, and may contain differences in details, or even bugs, that confound comparisons.

These factors motivate the development of a standardized software package which implements the most common metrics used to evaluate MIR systems.
Such a package should be straightforward to use and well-documented so that it can be easily adopted by MIR researchers.
In addition, it should be community-developed and transparently implemented so that all design decisions are easily understood and open to discussion and improvement.

\subsection{Chords}

Despite being one of the oldest MIREX tasks, evaluation methodology and metrics for automatic chord estimation is an ongoing topic of discussion, due to issues with vocabularies, comparison semantics, and other lexicographical challenges unique to the task~\cite{pauwels2013evaluating}.
One source of difficulty stems from an inherent subjectivity in ``spelling'' a chord name and the level of detail a human observer can provide in a reference annotation~\cite{ni2013understanding}.
As a result, a consensus has yet to be reached regarding the single best approach to comparing two sequences of chord labels, and instead are often compared over a set of rules, i.e Root, Major-Minor, and Sevenths, with or without inversions.

To efficiently compare chords, a given chord label is first separated into its
constituent parts, based on the syntax of~\cite{harte2010towards}.
For example, the chord label \texttt{G:maj(6)/5} is mapped to three pieces of information: the root (``G''), the root-invariant active semitones as determined by the quality shorthand (``maj'') and scale degrees (``6''), and the bass interval (``5'').
Based on this representation, an estimated chord label can be compare with a reference by defining a comparison function between these invariant representations.
Any encoded chords that are deemed to be ``out of gamut'' return a negative score to be easily filtered.

Track-wise scores are computed by weighting each comparison by the duration of its interval, over all intervals in an audio file.
This is achieved by forming the union of the boundaries in each sequence, sampling the labels, and summing the time intervals of the ``correct'' ranges.
The cumulative score, referred to as \emph{weighted chord symbol recall}, is tallied over a set audio files by discrete summation, where the importance of each score is weighted by the duration of each annotation~\cite{cho2013mirex}.


\section{Deep Learning for MIR}
\label{sec:dl4mir}

Shell scripts for feature extraction, training, and evaluation.
Results tabulated in iPython notebooks for reference and repeatability.

\subsection{Timbre}

Carrot cake marzipan gummies croissant oat cake pie candy canes chocolate.
Sugar plum jelly beans oat cake cake jujubes jelly chupa chups biscuit \ref{fig:muffin}.
Croissant cotton candy chupa chups.
Cheesecake tart bear claw brownie sugar plum.

\subsection{Chords}

Carrot cake marzipan gummies croissant oat cake pie candy canes chocolate.
Sugar plum jelly beans oat cake cake jujubes jelly chupa chups biscuit \ref{fig:muffin}.
Croissant cotton candy chupa chups.
Cheesecake tart bear claw brownie sugar plum.


\subsection{Guitar}

Carrot cake marzipan gummies croissant oat cake pie candy canes chocolate.
Sugar plum jelly beans oat cake cake jujubes jelly chupa chups biscuit \ref{fig:muffin}.
Croissant cotton candy chupa chups.
Cheesecake tart bear claw brownie sugar plum.


\section{Summary}
\label{sec:summary}

Everything necessary to repeat all research reported here is available online.
