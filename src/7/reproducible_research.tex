
% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- introduction file header -----------------------
% the code below specifies where the figures are stored
\graphicspath{{7/figures/}}

\chapter{Workflows for Reproducible Research}
\label{chp:reproducibility}

In recent years, the philosophy of open source software has begun taking root in scientific research, particularly in the field of computer science.
There are several reasons why open research is beneficial to the greater body of human knowledge, but three are of particular value here.
First, sharing code and data allows others to reproduce previous results, a fundamental tenet of the scientific method.
Open source implementations are invaluable for sufficiently complex systems.
It may be near impossible to describe every minute detail in a publication necessary for someone else to replicate the obtained results;
for some works, in fact, the only artifact that can do this unambiguously is the source code itself.
Second, and in a related vein, open research makes it both easier and faster to build upon and extend the previous work of others.
Even in the instance a researcher is able to recreate a published system, the time and effort necessary to get to this point is significant and arguably unnecessary.
Granted, while there is an educational component inherent to the re-implementation of previous work, the situation is akin to long division:
it is certainly valuable to learn \emph{how} to divide by hand, but no one shuns the use of a calculator on a day to day basis.
Lastly, it is a good and responsible act to contribute tools and software back to the larger research community.
All research stands on the shoulders of previous efforts, from improving on a recently published algorithm to the decades-old linear algebra routines doing all its number crunching.
The reality is that no one individual has ever truly solved anything on their own, and sharing the fruits of one's research endeavors serves the common good.

With these motivations in mind, this chapter details the several open source contributions made in the course of this work, culminating in a single code repository used to produce the results contained herein.
These software tools consist of the following:
Section \ref{sec:jams} describes \texttt{jams}, a JSON annotated music specification and python API for rich music descriptions;
Section \ref{sec:biggie} introduces \texttt{biggie}, an HDF5 interface for interacting with notoriously big data;
Section \ref{sec:optimus} details \texttt{optimus}, a user-friendly library for building and serializing arbitrary acyclic processing graphs;
Section \ref{sec:mir_eval} discusses relevant contributions to \texttt{mir\_eval}, a transparent framework for evaluating MIR systems;
and finally, Section \ref{sec:dl4mir} outlines \texttt{dl4mir}, the framework used here to complete this research.


\section{\texttt{jams}}
\label{sec:jams}

Music annotations ---the collection of observations made by one or more agents about an acoustic music signal--- are an integral component of content-based music informatics methodology, and are necessary for designing, evaluating, and comparing computational systems.
For clarity, the scope of an annotation is constrained to time scales at or below the level of a complete song, such as semantic descriptors (tags) or time-aligned chords labels.
Traditionally, the community has relied on plain text and custom conventions to serialize this data to a file for the purposes of storage and dissemination, collectively referred to as ``lab-files.''
Despite a lack of formal standards, lab-files have been, and continue to be, the preferred file format for a variety of music description tasks, such as beat or onset estimation, chord estimation, or segmentation.

Meanwhile, the interests and requirements of the community are continually evolving, thus testing the practical limitations of lab-files.
Reflecting on this tradition, there are three unfolding research trends that are demanding more of a given annotation format:

\begin{itemize}
\item \textbf{Comprehensive annotation data}:
Rich annotations, like the Billboard dataset\cite{Burgoyne2011expert}, require new, content-specific conventions, increasing the complexity of the software necessary to decode it and the burden on the researcher to use it; such annotations can be so complex, in fact, it becomes necessary to document how to understand and parse the format\cite{De2012parsing}.

\item \textbf{Multiple annotations for a given task}:
The experience of music can be highly subjective, at which point the notion of ``ground truth'' becomes tenuous.
Recent work in automatic chord estimation, both here and in \cite{ni2013understanding}, has shown that multiple reference annotations should be embraced, as they can provide important insight into both system evaluation and the problem formulation itself.

\item \textbf{Multiple concepts for a given signal}:
Although systems are classically developed to describe observations in a single namespace, e.g. chords, there is ongoing discussion toward integrating information across various musical concepts \cite{vincent2010roadmap}.
This has already yielded measurable benefits for the joint estimation of chords and downbeats \cite{papadopoulos2011joint} or chords and segments \cite{mauch2009using}, where leveraging multiple information sources for the same input signal can lead to improved performance.
\end{itemize}


\noindent It has long been acknowledged that lab-files cannot be used to these ends, and various formats and technologies have been previously proposed to alleviate these issues, such as RDF \cite{cannam2006sonic}, HDF5 \cite{bertin2011million}, or XML \cite{mckay2005ace}.
However, none of these formats have been widely embraced by the community.
Considering these options, the weak adoption of alternative formats is likely due to the combination of multiple factors.
For example, new tools can be difficult, if not impossible, to integrate into a research workflow because of compatibility issues with a preferred development platform or programming environment.
Additionally, it is a common criticism that the syntax or data model of these alternative formats is non-obvious, verbose, or otherwise confusing.
This is especially problematic when researchers must handle format conversions.
Therefore, a JSON Annotated Music Specification (JAMS) was developed to address these various needs.


\subsection{Core Design Principles}
\label{subsec:design}

In order to craft an annotation format that might serve the community into the foreseeable future, it is worthwhile to consolidate the lessons learned from both the relative success of lab-files and the challenges faced by alternative formats into a set of principles that might guide the design of a new format.
With this in mind, it is argued that usability, and thus the likelihood of adoption, is a function of three criteria:
simplicity, structure, and sustainability.

\subsubsection{Simplicity}
\label{subsubsec:simplicity}
The value of simplicity is demonstrated by lab-files in two specific ways.
First, the contents are represented in a format that is intuitive, such that the document model clearly matches the data structure and is human-readable, i.e. uses a lightweight syntax.
This is a particular criticism of RDF and XML, which can be verbose compared to plain text.
Second, lab-files are conceptually easy to incorporate into research workflows.
The choice of an alternative file format can be a significant hurdle if it is not widely supported, as is the case with RDF, or the data model of the document does not match the data model of the programming language, as with XML.

\subsubsection{Structure}
\label{subsubsec:structure}
It is important to recognize that lab-files developed as a way to serialize tabular data (i.e. arrays) in a language-independent manner.
Though lab-files excel at this particular use case, they lack the structure required to encode complex data such as hierarchies or mix different data types, such as scalars, strings, multidimensional arrays, etc.
This is a known limitation, and the community has devised a variety of ad hoc strategies to cope with it: folder trees and naming conventions, such as ``\{X\}/\{Y\}/\{Z\}.lab'', where X, Y, and Z correspond to ``artist'', ``album'', and ``title'', respectively\footnote{\url{http://www.isophonics.net/content/reference-annotations}}; parsing rules, such as ``lines beginning with `\#' are to be ignored as comments''; auxiliary websites or articles, decoupled from the annotations themselves, to provide critical information such as syntax, conventions, or methodology.
Alternative representations are able to manage more complex data via standardized markup and named entities, such as fields in the case of RDF or JSON, or IDs, attributes and tags for XML.

\subsubsection{Sustainability}
\label{subsubsec:sustainability}

Recently in MIR, a more concerted effort has been made toward sustainable research methods, which we see positively impacting annotations in two ways.
First, there is considerable value to encoding methodology and metadata directly in an annotation, as doing so makes it easier to both support and maintain the annotation while also enabling direct analyses of this additional information.
Additionally, it is unnecessary for the MIR community to develop every tool and utility ourselves; we should instead leverage well-supported technologies from larger communities when possible.

\subsection{The JAMS Schema}
\label{subsec:schema}

A JAMS object is hierarchically structured to capture all relevant information in a logically organized manner.
The primary record is an \emph{annotation}, of which a JAMS object may contain zero or more.
Annotations are comprised of \emph{observations}, which maintain a set of properties: time, duration, value, confidence, namespace.
Observations have two variants, to better handle \emph{sparse} or \emph{dense} data, such as onsets or melody, respectively.
The semantic context of an observation is specified by its \emph{namespace}, providing information about how the data in the observation should be understood.
Thus a namespace allows for easy filtering and interpretation of the data in an annotation for different music description tasks.

An annotation also maintains a \emph{metadata} object.
Annotation metadata allows for rich descriptions of \emph{who} and \emph{how} a particular record was produced.
Currently, metadata has properties such as \emph{corpus}, \emph{annotator}, \emph{validation}, \emph{curator}, to name a few fields.
Not only does this information make it easier to develop and evaluate systems with an eye to subjectivity, but it enables deeper meta-analyses of the annotations themselves.
This could be achieved by considering the observations made by annotators with different musical backgrounds or degrees of formal training, for example.

In addition to an array of annotations, JAMS objects also maintain a top-level file metadata object.
While annotation metadata sponges information about observer, file metadata tracks global information about the corresponding audio signal, with properties like \emph{title}, \emph{artist}, \emph{duration}, or \emph{indentifiers}.
As there is currently no standard convention for uniquely specifying audio recordings in a global manner, file metadata exists to help link the JAMS object with the appropriate signal.

Finally, \emph{sandboxes} exist in both the top-level and annotation objects to facilitate the growth and extensibility of the format.
These are unconstrained objects that can be used as needed for anything not covered by the formal schema.
This is done in the hope that sandboxes might identify information that could be incorporated into future versions.


\subsection{Python API}
\label{subsec:jams_tools}

To faciliate the use and integration of JAMS in software projects, a Python library is publicly available and under active development\footnote{\url{http://github.com/marl/jams}}.
This application programming interface (API) provides a simple interface for reading, writing, and manipulating the information in a JAMS object.
Many common datasets are also provided in this format to further encourage adoption.
Complementing the creation and use of human annotations, JAMS makes it easier to operate on this information, such as augmenting audio and annotations in parallel\footnote{\url{http://github.com/bmcfee/muda}}.


\section{\texttt{biggie}}
\label{sec:biggie}

Common practice in machine learning frameworks, like scikit-learn\footnote{\url{http://scikit-learn.org/}}, often proceeds by massaging the training data into something like a large table, i.e. rows are individual datapoints, and columns correspond to different features, coefficients, etc.
Attempts to map this paradigm to time-varying data, such as audio, can be problematic and cumbersome in practice.
It is typically advantageous to learn on several consecutive frames of features, referred to as tiles, windows, or shingles, as was the case in all work presented here.
These tiles could be sharded from longer feature sequences into some number of discrete, equivalently shaped observations, but this is generally undesirable.
Doing so limits the flexibility of the researcher considering different tile sizes, requiring that the data be sharded again.
More difficult, this practice increases the footprint of the dataset linearly with the size of each observation.
Lastly, it is helpful to keep the entire feature sequence in tact, as it facilitates the application of any additional time-dependent processing, e.g. Viterbi decoding.

To achieve these ends, \texttt{biggie}\footnote{\url{http://github.com/ejhumphrey/biggie}} is the high-dimensional, signal-level equivalent to JAMS, built on two basic objects.
An \emph{entity} is a struct-like object designed to keep various numerical representations together, regardless of samplerate.
These objects are then freely written to and read from a \emph{stash}, a persistent, i.e. on-disk, key-value store.
Here, each entity is given a unique key, making it easy to align a dictionary of signals with a dictionary of annotations.
Leveraging the h5py\footnote{\url{http://www.h5py.org/}} library under the hood and based on HDF5\footnote{\url{http://www.hdfgroup.org/HDF5/}}, biggie scales to arbitrarily large datasets;
however, though it shares a common interface with in-memory dictionaries, the footprint of a stash scales gracefully with available computational resources by lazily loading data into memory.
Biggie also allows random access and data caching, greatly facilitating stochastic sampling for on-line learning\footnote{\url{https://github.com/bmcfee/pescador}}.
Perhaps most practically though, biggie serves as \emph{the} data interchange format between processing stages in the larger framework, eliminating the need for filename parsing or content-specific naming conventions; operations are written to consume and return stashes with data under the same keys.
While biggie helps address similar pain points as other libraries, like \texttt{pandas}\footnote{\url{http://pandas.pydata.org/}}, the HDF5 backend is crucial for serializing a large collection numerical tensors in a simple, Pythonic manner.


\section{\texttt{optimus}}
\label{sec:optimus}

% What is the problemo
Deep learning tools have matured rapidly in the last half decade, with powerful choices across several programming languages.
Developed under the direction of Yoshua Bengio at the University of Montreal, Theano\footnote{\url{http://deeplearning.net/software/theano/}}, is the leading Python library for deep learning.
Boasting a large and growing user base, Theano offers all the pieces necessary for deep learning research, including symbolic differentiation, optimized C-implementations, and seamless integration with GPUs.
Though extremely powerful, useful, and expressive, there are two facets of the library that proved troublesome in the course of this work.
Serialization is can be tricky, especially for networks under active development.
The common approach to saving objects in Python, known as ``pickling'', is sensitive to modifications to the code, and thus something pickled previously may not be recoverable in the future.
Additionally, designing neural networks in Theano can be rather time-consuming, and is not the most user-friendly experience.
This is especially true when constructing non-standard architectures, such as guitar fretboard models or pairwise training strategies.

% How does optimus solve these problems
Optimus\footnote{\url{http://github.com/ejhumphrey/optimus}} is therefore an effort to address both of these problems in a maximally versatile and intuitive manner.
To simplify the creation, training, and application of neural networks, common building blocks are provided as natively serializable objects that can be wired together in a graphical manner.
% Nodes and math
Arbitrary acyclic, i.e. no loops, directed processing graphs can be architected from a large collection of \emph{nodes}, including inputs, functions, like ``Affine'' or ``Conv3D'', and outputs.
% Losses
While a large space of loss functions can be realized from these basic building blocks, a handful of standard \emph{losses} are provided, simplifying design further.
The topology between these parts is given by a routing table, and passed off to a \emph{graph}, which connects the dots and returns a callable function.
Furthermore, given the flexibility to define topologies in a processing graph, it is simple to explore a variety of modifications, such as layerwise hyperparameters, tapping various intermediary representations as outputs, or ``cloning'' a processing node to create a deep copy of its parameters.

In addition to the user-facing advantages, robust serialization is achieved by expressing processing graph definitions as JSON and archives of multidimensional arrays.
This offers the additional benefit that it would be straightforward to port optimus models, as graph definitions and parameter bundles, across languages in the future.
Finally, as parameter assignments are expressed through named nodes and fields, it is trivial to not only save parameters but easily initialize them by arbitrary means, such as pre-training or using supervised learning on hand-crafted functions.


\section{\texttt{mir\_eval}}
\label{sec:mir_eval}

As addressed at the outset of this work, the conventional formulation of many music description tasks attempts to model the behavior of an expert human listener.
Framing the problem as a mapping between inputs and outputs allows for objective quantitative evaluation, thus providing common dimensions on which to compare algorithms and systems against each other.
The particular dimensions on which an algorithm is evaluated is almost by definition specific to the application, and different metrics have evolved over time to provide musically meaningful assessment.
Many such scoring functions are nontrivial to implement, however, and small details can give rise to variability in resulting metrics.
The Music Information Retrieval Evaluation eXchange (MIREX), the community's annual algorithm evaluation event, has helped provide common ground on which to compare different systems.
This implementation is seldom used outside MIREX due to a variety of practical difficulties, however, and instead, researchers often resort to reimplementing the same evaluation metrics.
Unfortunately these personal implementations are not standardized, and may differ in important details, or even contain bugs, confounding comparisons.
Therefore, the \texttt{mir\_eval} project aims to address these challenges for a variety of well-established tasks \cite{Raffel2014}.


Though larger in scope, the contributions to the chord estimation evaluation are particularly relevant to this work.
Despite being one of the oldest MIREX tasks, evaluation methodology and metrics for automatic chord estimation is an ongoing topic of discussion, due to issues with vocabularies, comparison semantics, and other lexicographical challenges unique to the task~\cite{pauwels2013evaluating}.
One source of difficulty stems from an inherent subjectivity in ``spelling'' a chord name and the level of detail a human observer can provide in a reference annotation~\cite{ni2013understanding}.
As a result, a consensus has yet to be reached regarding the single best approach to comparing two sequences of chord labels, and instead are often compared over a set of rules, i.e Root, Major-Minor, and Sevenths, with or without inversions.

To efficiently compare chords, a given chord label is first separated into its
constituent parts, based on the syntax of~\cite{harte2010towards}.
For example, the chord label \texttt{G:maj(6)/5} is mapped to three pieces of information: the root (``G''), the root-invariant active semitones as determined by the quality shorthand (``maj'') and scale degrees (``6''), and the bass interval (``5'').
Based on this representation, an estimated chord label can be compare with a reference by defining a comparison function between these invariant representations.
Any encoded chords that are deemed to be ``out of gamut'' return a negative score to be easily filtered.
Track-wise scores are computed by weighting each comparison by the duration of its interval, over all intervals in an audio file.
This is achieved by forming the union of the boundaries in each sequence, sampling the labels, and summing the time intervals of the ``correct'' ranges.
The cumulative score, referred to as \emph{weighted chord symbol recall}, is tallied over a set audio files by discrete summation, where the importance of each score is weighted by the duration of each annotation~\cite{cho2013mirex}.


\section{\texttt{dl4mir}}
\label{sec:dl4mir}

Leveraging these various software components, an aggregate framework representing all work presented herein is also made available, referred to as \texttt{dl4mir}\footnote{\url{http://github.com/ejhumphrey/dl4mir}}.
At a high level, this resource contains the additional functionality necessary to reproduce this work.
For convenience, the majority of processes can be executed via a series of shell scripts to perform feature extraction, training, and evaluation.
All reported results and figures provided in this document are survived in IPython notebooks for both reference and repeatability.
Furthermore, dependencies have been minimized to make this framework sufficiently independent.
% Goals
In addition to the immediate goal of reproducing experimental results, this is done in the hope that it may facilitate future extensions of this work.
Serialized versions of important network models are also included to make comparisons against similar work easier, or to simply build them into other systems.
Finally, the source code is provided so that it will serve as another example of how one might architect a flexible deep learning framework.


\section{Summary}
\label{sec:summary}

This chapter has covered the various libraries and programming tools developed in the course of this work:
\texttt{jams}, a JSON format and Python API for music annotation;
\texttt{biggie}, an HDF5 interface for managing large amounts of numerical data;
\texttt{optimus}, a versatile yet intuitive approach to building, training, and saving deep networks;
and \texttt{mir\_eval}, an open framework for evaluation.
Not only are many of these components independently useful to deep learning workflows, but the software necessary to repeat the research reported here is made publicly available online, in the form of \texttt{dl4mir}.
Following the spirit of reproducible research, these efforts aspire to make it easier to repeat, compare against, and extend the work presented, ultimately serving the greater music informatics community.
